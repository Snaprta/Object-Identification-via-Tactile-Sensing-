# -*- coding: utf-8 -*-
"""Tactile_Perception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KOsE8zJfgA667XYBkYAn799MveSws_gy
"""
import warnings

# Suppress FutureWarnings from scikit-learn
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn import tree
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.mixture import GaussianMixture
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
import pydot
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification
from sklearn.svm import SVC

# dataset
data = pd.read_csv('AllDataMLnoFS.csv')
# data = pd.read_csv('dataset1.csv')

#Get the features and labels
# X =data.loc[:,data.columns!='Terrain'].values[:,1:]
# Y =data.loc[:,'Terrain'].values
X =data.loc[:,data.columns!='Fruit'].values[:,1:]
Y =data.loc[:,'Fruit'].values
data.describe()

"""1. Random Forest"""
print("Random Forest")
# now lets split the data into train and test
from sklearn.model_selection import train_test_split
# Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, random_state=500)
X_train.shape, X_test.shape

from sklearn.ensemble import RandomForestClassifier
classifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5,
                                       n_estimators=100, oob_score=True)

classifier_rf.fit(X_train, y_train)

# checking the oob score
y_pred = classifier_rf.predict(X_test)
acc = classifier_rf.score(X_test, y_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)
classifier_rf.oob_score_
print('OOB %.2f' %classifier_rf.oob_score_)


# Export the first three decision trees from the forest

"""2. Decision Tree"""
print("\nDecision Tree")
# importing train test split
from sklearn.model_selection import train_test_split

# creating a train-test split
X_train, X_test, Y_train, Y_test = train_test_split( X, Y, 
                    test_size = 0.2, random_state = 1 )

DT = tree.DecisionTreeClassifier()
DT.fit(X_train, Y_train)
Y_pred = DT.predict(X_test)
acc = DT.score(X_test, Y_test)

print('Accuracy: %.2f' % acc)
# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import mean_squared_error, r2_score

# The mean squared error
mean_square = mean_squared_error(Y_test, Y_pred)
print('Mean squared error: %.2f' % mean_square)

# The coefficient of determination: 1 is perfect prediction
r2 = r2_score(Y_test, Y_pred)
print('Coefficient of determination: %.2f' % r2)


"""3. Logistic Regression"""
print("\nLogistic Regression")
# importing train test split
from sklearn.model_selection import train_test_split

# creating a train-test split
X_train, X_test, Y_train, Y_test = train_test_split( X, Y, 
                    test_size = 0.4, random_state = 1 )

# printing size of train and test data

print('X_train : ', X_train.shape)
print('X_test : ', X_test.shape)

print('Y_train : ', Y_train.shape)
print('Y_test : ', Y_test.shape)

LR = LogisticRegression(random_state=0, max_iter=5000, solver = 'lbfgs' )
LR.fit(X_train, Y_train)
Y_pred = LR.predict(X_test)
print('Coefficients: \n', LR.coef_)

# accuracy 
print('Accuracy on Train : ', round(LR.score(X_train, Y_train)*100, 2))
print('Accuracy on Test : ', round(LR.score(X_test, Y_test)*100, 2))
print('Accuracy on Whole Dataset : ', round(LR.score(X, Y)*100, 2))

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Confusion Matrix 
result = confusion_matrix(Y_test, Y_pred)
print("Confusion Matrix:")
print(result)

# Classification report
result1 = classification_report(Y_test, Y_pred)
print("\nClassification Report:")
print (result1)

# Accuracy score
result2 = accuracy_score(Y_test, Y_pred)
print("\nAccuracy:",result2)

"""4. K Means"""
print('K Means')
#Let arbitary K=5
kmeans5 = KMeans(n_clusters=5)
y_kmeans5 = kmeans5.fit_predict(X)

#Elbow Plot

Error =[]
for i in range(1, 201):
    kmeans = KMeans(n_clusters = i).fit(X)
    kmeans.fit(X)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 201), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()

#Using 4 as our optimal K
kmeans4 = KMeans(n_clusters=3)
y_kmeans4 = kmeans4.fit_predict(X)

plt.scatter(X[:, 0], X[:, 1], c=y_kmeans4, cmap='rainbow' )

# importing train test split
from sklearn.model_selection import train_test_split

# creating a train-test split
X_train, X_test, Y_train, Y_test = train_test_split( X, Y, 
                    test_size = 0.4, random_state = 1 )

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
knnr = KNeighborsRegressor(n_neighbors = 2)
knnr.fit(X_train, Y_train)

X_train.shape
# Y_train.shape
# Y_test.shape

# Y_train = Y_train.astype(float)
# Y_test = Y_test.astype(float)

# accuracy
print('Accuracy on Train : ', round(knnr.score(X_train, Y_train)*100, 2))
print('Accuracy on Test : ', round(knnr.score(X_test, Y_test)*100, 2))

"""5. SVM"""
print("SVM")
#Scale the features to between -1 and 1
scaler=MinMaxScaler((-1,1))
x=scaler.fit_transform(data)
y=data

# importing train test split
from sklearn.model_selection import train_test_split

# creating a train-test split
x_train, x_test, y_train, y_test = train_test_split( x, y, 
                    test_size = 0.3, random_state = 5)

# printing size of train and test data
print('X_train : ', x_train.shape)
print('X_test : ', x_test.shape)

print('Y_train : ', y_train.shape)
print('Y_test : ', y_test.shape)

# Types of kernels : ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’
svm = SVC(kernel = 'sigmoid')

svm = GradientBoostingClassifier()
svm.fit(X_train,Y_train)

#Accuracy
y_pred=svm.predict(X_test)
print('\nAccuracy : ', accuracy_score(Y_test, y_pred)*100)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Confusion Matrix 
result = confusion_matrix(Y_test, Y_pred)
print("\nConfusion Matrix:")
print(result)

# Classification report
result1 = classification_report(Y_test, Y_pred)
print("\nClassification Report:")
print (result1)

# Accuracy score
result2 = accuracy_score(Y_test, Y_pred)
print("\nAccuracy:",result2)

"""6. MLP"""
print("\nMLP")

data['Fruit'].value_counts()
x = data.drop('Fruit', axis=1)
y = data['Fruit']
trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)
sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)

mlp_clf = MLPClassifier(hidden_layer_sizes=(4,150,200,50,11),
                        max_iter = 1000,activation = 'relu',
                        solver = 'adam')

mlp_clf.fit(trainX_scaled, trainY)

y_pred = mlp_clf.predict(testX_scaled)
accuracy_score
print('Test Accuracy: {:.2f}'.format(accuracy_score(testY, y_pred)*100))
#F1 score

from sklearn.metrics import confusion_matrix
import seaborn as sns

conf=confusion_matrix(testY,y_pred)

df_cm = pd.DataFrame(conf, columns=np.unique(testY), index = np.unique(testY))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (14,14))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 12})# font size

print(classification_report(testY, y_pred))

plt.plot(mlp_clf.loss_curve_)
plt.title("Loss Curve", fontsize=14)
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.show()

"""7. 1D CNN"""

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

import tensorflow as tf

from tensorflow import keras

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout

from sklearn.preprocessing import StandardScaler


# Load the dataset


# Split the data into features (X) and labels (y)

X = data.iloc[:, 1:].values

y = data.iloc[:, 0].values


# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Standardize features

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)


# Reshape data for 1D CNN (assuming 3 sensor readings per sample)

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)

X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)


# Create a 1D CNN model

model = Sequential()

model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1), padding='same'))

model.add(MaxPooling1D(pool_size=1))  # Reduce pooling size

model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))

model.add(MaxPooling1D(pool_size=1))  # Reduce pooling size

model.add(Flatten())

model.add(Dense(128, activation='relu'))

model.add(Dropout(0.5))

model.add(Dense(10, activation='softmax'))


# Compile the model
from tensorflow.keras.callbacks import EarlyStopping

# Create an EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])






# Train the model

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])


# Plot training history

plt.plot(history.history['accuracy'], label='accuracy')

plt.plot(history.history['val_accuracy'], label='val_accuracy')

plt.xlabel('Epoch')

plt.ylabel('Accuracy')

plt.legend(loc='lower right')

plt.show()

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Test Loss')
plt.legend()
plt.show()

# Train the model

# Plot training history (Your existing code)

# Make predictions on the test set
predictions = model.predict(X_test)

# Convert predictions to class labels
predicted_labels = np.argmax(predictions, axis=1)

# Print some example predictions and true labels
num_examples_to_print = 10
for i in range(num_examples_to_print):
    print(f"Example {i + 1}: Predicted={predicted_labels[i]}, True={y_test[i]}")

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

print(classification_report(y_test, predicted_labels))